{{- if .Values.notebook.enabled }}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "fraud-detection.fullname" . }}-notebooks
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "fraud-detection.labels" . | nindent 4 }}
    app.kubernetes.io/component: notebook
data:
  QUICKSTART.md: |
    # Fraud Detection with LakeFS - Quick Start
    
    Welcome to your Jupyter notebook environment! This workspace is pre-configured for fraud detection experiments with LakeFS data versioning.
    
    ## ðŸš€ Getting Started
    
    ### 1. Check Your Environment
    
    Run this in a notebook cell to verify connections:
    
    ```python
    import os
    
    # LakeFS connection
    print(f"LakeFS Endpoint: {os.getenv('LAKEFS_ENDPOINT')}")
    print(f"LakeFS Access Key: {os.getenv('LAKEFS_ACCESS_KEY_ID')}")
    
    # MinIO connection
    print(f"S3 Endpoint: {os.getenv('AWS_S3_ENDPOINT')}")
    print(f"S3 Access Key: {os.getenv('AWS_ACCESS_KEY_ID')}")
    ```
    
    ### 2. Install Required Packages
    
    ```python
    # Install LakeFS client
    !pip install lakefs-client boto3 pandas scikit-learn
    
    # For ML workloads
    !pip install tensorflow torch transformers
    ```
    
    ### 3. Connect to LakeFS
    
    ```python
    import lakefs_client
    from lakefs_client.client import LakeFSClient
    
    # Configuration is already set via environment variables
    configuration = lakefs_client.Configuration(
        host=os.getenv('LAKEFS_ENDPOINT'),
        username=os.getenv('LAKEFS_ACCESS_KEY_ID'),
        password=os.getenv('LAKEFS_SECRET_ACCESS_KEY')
    )
    
    client = LakeFSClient(configuration)
    
    # List repositories
    repos = client.repositories.list_repositories()
    print(f"Available repositories: {[repo.id for repo in repos]}")
    ```
    
    ## ðŸ“š Available Notebooks
    
    Navigate to `demo/notebooks/` to find:
    
    1. **1_experiment_train_lakefs.ipynb** - Train models with LakeFS versioning
    2. **2_save_model_lakefs.ipynb** - Save and version ML models
    3. **3_rest_requests_multi_model_lakefs.ipynb** - Multi-model REST inference
    4. **4_grpc_requests_multi_model_lakefs.ipynb** - High-performance gRPC inference
    5. **5_rest_requests_single_model_lakefs.ipynb** - Single model deployment
    6. **8_distributed_training_lakefs.ipynb** - Distributed training patterns
    
    ## ðŸ”§ Pre-configured Services
    
    | Service | Endpoint | Purpose |
    |---------|----------|---------|
    | LakeFS | {{ .Values.notebook.lakefs.url }} | Data versioning & management |
    | MinIO | {{ .Values.notebook.minio.endpoint }} | S3-compatible object storage |
    
    ## ðŸ’¡ Tips
    
    - **Save frequently** - Your work is persisted on the PVC
    - **Use LakeFS branches** - Create branches for experiments
    - **Version your data** - Commit data changes to LakeFS
    - **Monitor resources** - Check memory/CPU in the status bar
    
    ## ðŸ“– Learn More
    
    - [LakeFS Documentation](https://docs.lakefs.io/)
    - [Chart README](https://github.com/rh-aiservices-bu/Fraud-Detection-data-versioning-with-lakeFS/blob/main/deploy/helm/fraud-detection/README.md)
    - [OpenShift AI Documentation](https://access.redhat.com/documentation/en-us/red_hat_openshift_ai/)
    
    ## ðŸ†˜ Need Help?
    
    Check the environment variables:
    ```bash
    env | grep -E '(LAKEFS|AWS)' | sort
    ```
    
    Test connectivity:
    ```python
    import requests
    response = requests.get(os.getenv('LAKEFS_ENDPOINT') + '/api/v1/healthcheck')
    print(f"LakeFS Status: {response.status_code}")
    ```
    
    ---
    
    **Happy Experimenting! ðŸŽ‰**
{{- end }}

