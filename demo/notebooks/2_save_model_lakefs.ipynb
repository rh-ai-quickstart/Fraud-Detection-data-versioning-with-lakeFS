{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model\n",
    "\n",
    "To save this model so that you can use it from various locations, including other notebooks or the model server, upload it to s3-compatible storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install the required packages and define a function for the upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3 botocore lakefs==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define lakeFS Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import lakefs\n",
    "repo_name = os.environ.get('LAKEFS_REPO_NAME')\n",
    "\n",
    "mainBranch = \"main\"\n",
    "trainingBranch = \"train01\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# lakeFS CONTROL PLANE: Reference the repository (versioned AI namespace)\n",
    "# What this does: Creates a client-side handle to the lakeFS repository that\n",
    "# versions all datasets and model artifacts for this workflow.\n",
    "# Why it matters: The repository is the control-plane boundary where lineage,\n",
    "# commits, and promotions are tracked across training and serving.\n",
    "# AI data control plane value: Establishes a single governed namespace tying\n",
    "# model artifacts back to the exact data versions used to produce them.\n",
    "# -----------------------------------------------------------------------------\n",
    "repo = lakefs.Repository(repo_name)\n",
    "print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "\n",
    "aws_access_key_id = os.environ.get('LAKECTL_CREDENTIALS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.environ.get('LAKECTL_CREDENTIALS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('LAKECTL_SERVER_ENDPOINT_URL')\n",
    "region_name = os.environ.get('LAKEFS_DEFAULT_REGION')\n",
    "bucket_name = os.environ.get('LAKEFS_REPO_NAME')\n",
    "\n",
    "if not all([aws_access_key_id, aws_secret_access_key, endpoint_url, region_name, bucket_name]):\n",
    "    raise ValueError(\"One or data connection variables are empty.  \"\n",
    "                     \"Please check your data connection to an S3 bucket.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# lakeFS DATA PLANE (S3 gateway configuration)\n",
    "# What this does: Configures boto3 to send standard S3 API calls to the lakeFS\n",
    "# endpoint rather than directly to object storage.\n",
    "# Why it matters: All object uploads still use native S3 semantics, but lakeFS\n",
    "# transparently versions these writes and associates them with branches/commits.\n",
    "# AI data control plane value: Preserves compatibility with existing ML tooling\n",
    "# while enabling versioned, auditable model storage.\n",
    "# -----------------------------------------------------------------------------\n",
    "session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "s3_resource = session.resource(\n",
    "    's3',\n",
    "    config=botocore.client.Config(signature_version='s3v4'),\n",
    "    endpoint_url=endpoint_url,\n",
    "    region_name=region_name)\n",
    "\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "\n",
    "\n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            bucket.upload_file(file_path, s3_key)\n",
    "            num_files += 1\n",
    "    return num_files\n",
    "\n",
    "\n",
    "def list_objects(prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Verify the upload\n",
    "\n",
    "In your S3 bucket, under the `models` upload prefix, run the `list_object` command. As best practice, to avoid mixing up model files, keep only one model and its required files in a given prefix or directory. This practice allows you to download and serve a directory with all the files that a model requires. \n",
    "\n",
    "If this is the first time running the code, this cell will have no output.\n",
    "\n",
    "If you've already uploaded your model, you should see this output: `models/fraud/1/model.onnx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_objects(f\"{trainingBranch}/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload the model to the training branch in lakeFS and check again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to upload the `models` folder in a rescursive fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_models_directory = \"models\"\n",
    "\n",
    "if not os.path.isdir(local_models_directory):\n",
    "    raise ValueError(f\"The directory '{local_models_directory}' does not exist.  \"\n",
    "                     \"Did you finish training the model in the previous notebook?\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# lakeFS: Upload model artifacts into a branch namespace\n",
    "# What this does: Uploads the trained model files into the trainingBranch path\n",
    "# (s3://<repo>/<branch>/models/...).\n",
    "# Why it matters: Writing into a branch path stages model changes without\n",
    "# impacting the main branch used for serving or other experiments.\n",
    "# AI data control plane value: Enables safe, isolated model experimentation and\n",
    "# validation before promotion, using standard S3 tooling.\n",
    "# -----------------------------------------------------------------------------\n",
    "num_files = upload_directory_to_s3(\"models\", f\"{trainingBranch}/models\")\n",
    "\n",
    "if num_files == 0:\n",
    "    raise ValueError(\"No files uploaded.  Did you finish training and \"\n",
    "                     \"saving the model to the \\\"models\\\" directory?  \"\n",
    "                     \"Check for \\\"models/fraud/1/model.onnx\\\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To confirm this worked, run the `list_objects` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# lakeFS: Inspect objects staged on a branch\n",
    "# What this does: Lists objects currently present under the trainingBranch path.\n",
    "# Why it matters: Provides immediate visibility into which model artifacts are\n",
    "# part of this branch before committing or promoting them.\n",
    "# AI data control plane value: Makes versioned model state explicit and reviewable\n",
    "# prior to creating an immutable snapshot.\n",
    "# -----------------------------------------------------------------------------\n",
    "list_objects(f\"{trainingBranch}/models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit changes in lakeFS repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# lakeFS CONTROL PLANE: Commit model artifacts (immutable snapshot)\n",
    "# What this does: Creates a commit capturing the current contents of the\n",
    "# trainingBranch, including the uploaded model files.\n",
    "# Why it matters: Commits are immutable references. This guarantees that the\n",
    "# model artifact can always be traced back to a specific, reproducible state.\n",
    "# AI data control plane value: Enables precise lineage between training data,\n",
    "# preprocessing artifacts, and the model used for serving or evaluation.\n",
    "# -----------------------------------------------------------------------------\n",
    "branchTraining = repo.branch(trainingBranch)\n",
    "ref = branchTraining.commit(message='Uploaded data, artifacts and model')\n",
    "print(ref.get_commit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "Now that you've saved the model to s3 storage, you can refer to the model by using the same data connection to serve the model as an API.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
